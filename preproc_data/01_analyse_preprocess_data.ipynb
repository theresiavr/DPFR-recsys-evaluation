{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it works:\n",
    "- Execute run.py to get the initial train, val, test splits (saved in the `train_val_test` folder)\n",
    "- Move the files to the folder train_val_test_before_remove_train\n",
    "- This notebook makes use of those splits and preprocess them further.\n",
    "- We save the further preprocessed datasets as ''benchmark files'' for input to RecBole. Those will be our final datasets that we will use for training, validation, and test.\n",
    "- Execute run_new.py to populate the `train_val_test` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_df(df, dataset_name):\n",
    "    print(\"Getting stats from these columns: \", df.columns[0:2])\n",
    "    num_user = df.iloc[:,0].unique().shape[0]\n",
    "    num_item = df.iloc[:,1].unique().shape[0]\n",
    "    num_inter = len(df)\n",
    "    sparsity = 1 - num_inter / (num_user * num_item)\n",
    "    print(\"Statistics: \")\n",
    "    print(f\"Number of users: {num_user}\")\n",
    "    print(f\"Number of items: {num_item}\")\n",
    "    print(f\"Number of interactions: {num_inter}\")\n",
    "    print(f\"Sparsity: {sparsity}\")\n",
    "\n",
    "    return {dataset_name: [num_user, num_item, num_inter, sparsity]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    print(f\"Loading {dataset_name}\")\n",
    "    df = pd.read_csv(f\"../dataset/{dataset_name}/{dataset_name}.inter\", sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def load_preprocessed_data(dataset, path=\"train_val_test\"):\n",
    "\n",
    "    with open(f\"../{path}/{dataset}_train.pickle\",\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    train = pd.DataFrame(data)\n",
    "\n",
    "    with open(f\"../{path}/{dataset}_valid.pickle\",\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    val = pd.DataFrame(data)\n",
    "\n",
    "    with open(f\"../{path}/{dataset}_test.pickle\",\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    test = pd.DataFrame(data)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def concat(train, val, test):\n",
    "    return pd.concat([train, val, test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataset = [\"Amazon-lb\",\"Jester\",\"Lastfm\",\"ML-10M\", \"QK-video\", \"ML-20M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def further_preprocess(dataset_name):\n",
    "\n",
    "        train, val, test = load_preprocessed_data(dataset_name, \"train_val_test_before_remove_train\")\n",
    "\n",
    "        #Remove users in train with less than 5 interactions (only keep those with at least 5)\n",
    "\n",
    "        new_train_agg = train\\\n",
    "                .groupby(\"user_id\")\\\n",
    "                .count()\n",
    "        new_train_id = new_train_agg[new_train_agg.iloc[:, 0]>=5].index\n",
    "        new_train = train[train.user_id.isin(new_train_id)]\n",
    "\n",
    "        #Completely remove those users in val and test\n",
    "        new_val = val[val.user_id.isin(new_train_id)]\n",
    "        new_test =  test[test.user_id.isin(new_train_id)]\n",
    "\n",
    "        #Ensure all val and test users are in new train\n",
    "        assert new_val.user_id.isin(new_train_id).all()\n",
    "        assert new_test.user_id.isin(new_train_id).all()\n",
    "\n",
    "        #ensure each user in train has at least 5\n",
    "        assert all(new_train\\\n",
    "                .groupby(\"user_id\")\\\n",
    "                .count()\\\n",
    "                .iloc[:, 0] >= 5)\n",
    "\n",
    "        return new_train, new_val, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_inter(df:pd.DataFrame, col_name_dict:dict, file_name:str, dataset_name):\n",
    "    inter = df.copy()\n",
    "    inter.rename(columns=col_name_dict, inplace=True)\n",
    "\n",
    "    path = f\"../preproc_data/new_{dataset_name}/\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    inter.to_csv(path+file_name, index=False, sep=\"\\t\")\n",
    "    return inter\n",
    "\n",
    "def create_file(dataset_name):\n",
    "    train, val, test = further_preprocess(dataset_name)\n",
    "\n",
    "    col_name_dict = {\n",
    "                \"user_id\":\"user_id:token\",\n",
    "                \"item_id\":\"item_id:token\",\n",
    "                \"artist_id\":\"artist_id:token\",\n",
    "                \"label\":\"label:float\",\n",
    "                \"timestamp\":\"timestamp:float\"\n",
    "                }\n",
    "    \n",
    "    #this method converts our loaded dataframe to a .inter file, and saves it in the folder data under the name 'file_name'\n",
    "    convert_df_to_inter(train, col_name_dict, f\"new_{dataset_name}.train.inter\", dataset_name)\n",
    "    convert_df_to_inter(val, col_name_dict,f\"new_{dataset_name}.valid.inter\", dataset_name)\n",
    "    convert_df_to_inter(test, col_name_dict, f\"new_{dataset_name}.test.inter\", dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented to avoid accidental run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in list_dataset:\n",
    "#     create_file(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Stats\n",
    "Count number of user, item, interaction.\n",
    "\n",
    "This can be run after the instruction at the beginning of the notebook has been done (including run_new.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataset = [\"Amazon-lb\",\"Jester\",\"Lastfm\",\"ML-10M\",\"ML-20M\",\"QK-video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "old_preproc_result = {}\n",
    "preproc_result = {}\n",
    "train_val_test_result = {}\n",
    "test_df = {}\n",
    "\n",
    "for data in list_dataset:\n",
    "    df = load_data(data)\n",
    "    stat_data = stat_df(df, data)\n",
    "    result.update(stat_data)\n",
    "\n",
    "    #old\n",
    "    old_train, old_val, old_test = load_preprocessed_data(data, \"train_val_test_before_remove_train\")\n",
    "    old_preproc_data = concat(old_train,old_val,old_test)\n",
    "\n",
    "    old_preproc_stat_data = stat_df(old_preproc_data, data)\n",
    "    old_preproc_result.update(old_preproc_stat_data)\n",
    "\n",
    "    #new\n",
    "    train, val, test = load_preprocessed_data(\"new_\"+data,  \"train_val_test\")\n",
    "    preproc_data = concat(train,val,test)\n",
    "\n",
    "    preproc_stat_data = stat_df(preproc_data, \"new_\"+data)\n",
    "    preproc_result.update(preproc_stat_data)\n",
    "    test_df[data] = test\n",
    "\n",
    "    #per split\n",
    "    for i, (old_split, new_split) in enumerate(zip([old_train, old_val, old_test],[train, val, test])):\n",
    "        old_stat = stat_df(old_split, data)\n",
    "        new_stat = stat_df(new_split, \"new_\"+data)\n",
    "        if i not in train_val_test_result:\n",
    "            train_val_test_result[i] = old_stat\n",
    "            train_val_test_result[i].update(new_stat)\n",
    "        else:\n",
    "            train_val_test_result[i].update(old_stat)\n",
    "            train_val_test_result[i].update(new_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(result).T\n",
    "df_result.columns = [\"num_user\", \"num_item\", \"num_inter\", \"sparsity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index = df_result.sort_values(\"num_inter\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_user</th>\n",
       "      <th>num_item</th>\n",
       "      <th>num_inter</th>\n",
       "      <th>sparsity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>new_Lastfm</th>\n",
       "      <td>1842.0</td>\n",
       "      <td>2823.0</td>\n",
       "      <td>71243.0</td>\n",
       "      <td>0.986299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_Amazon-lb</th>\n",
       "      <td>1054.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>12397.0</td>\n",
       "      <td>0.985130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_QK-video</th>\n",
       "      <td>4656.0</td>\n",
       "      <td>6423.0</td>\n",
       "      <td>51777.0</td>\n",
       "      <td>0.998269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_Pinterest</th>\n",
       "      <td>55173.0</td>\n",
       "      <td>9607.0</td>\n",
       "      <td>1407412.0</td>\n",
       "      <td>0.997345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_Jester</th>\n",
       "      <td>63724.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2150060.0</td>\n",
       "      <td>0.662598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_Anime</th>\n",
       "      <td>56867.0</td>\n",
       "      <td>7833.0</td>\n",
       "      <td>6102246.0</td>\n",
       "      <td>0.986301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_ML-10M</th>\n",
       "      <td>49378.0</td>\n",
       "      <td>9821.0</td>\n",
       "      <td>5362685.0</td>\n",
       "      <td>0.988942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_ML-20M</th>\n",
       "      <td>89917.0</td>\n",
       "      <td>16404.0</td>\n",
       "      <td>10588141.0</td>\n",
       "      <td>0.992822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               num_user  num_item   num_inter  sparsity\n",
       "new_Lastfm       1842.0    2823.0     71243.0  0.986299\n",
       "new_Amazon-lb    1054.0     791.0     12397.0  0.985130\n",
       "new_QK-video     4656.0    6423.0     51777.0  0.998269\n",
       "new_Pinterest   55173.0    9607.0   1407412.0  0.997345\n",
       "new_Jester      63724.0     100.0   2150060.0  0.662598\n",
       "new_Anime       56867.0    7833.0   6102246.0  0.986301\n",
       "new_ML-10M      49378.0    9821.0   5362685.0  0.988942\n",
       "new_ML-20M      89917.0   16404.0  10588141.0  0.992822"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_result = pd.DataFrame(preproc_result).T\n",
    "preproc_result.columns = [\"num_user\", \"num_item\", \"num_inter\", \"sparsity\"]\n",
    "preproc_result.loc[\"new_\"+sorted_index].to_excel(\"stats/new_dataset_statistics_preprocessed.xlsx\")\n",
    "preproc_result.loc[\"new_\"+sorted_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(train_val_test_result)\\\n",
    "        .rename(columns={0:\"train\",1:\"val\",2:\"test\"})\\\n",
    "        .applymap(lambda x: x[:-1]).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table for num of relevant items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_item_df = pd.DataFrame()\n",
    "\n",
    "for data, df in test_df.items():\n",
    "    rel_item_per_user = df.groupby(\"user_id\").agg(lambda x: {x for x in x}).reset_index()\n",
    "    col_name = rel_item_per_user.drop(columns=[\"user_id\"]).columns[0]\n",
    "    print(col_name)\n",
    "    to_concat = pd.DataFrame(\n",
    "                rel_item_per_user[col_name]\\\n",
    "                        .apply(len)\\\n",
    "                        .describe()\\\n",
    "                        .loc[[\"mean\",\"min\",\"50%\",\"max\"]]).T\\\n",
    "                        .rename(index={col_name:data}, columns={\"50%\":\"median\"})\\\n",
    "                        .round(2)\n",
    "\n",
    "    rel_item_df = pd.concat([rel_item_df, to_concat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_item_df[[\"min\",\"median\",\"max\"]] = rel_item_df[[\"min\",\"median\",\"max\"]].astype(int)\n",
    "rel_item_df = rel_item_df.loc[[\"Lastfm\", \"Amazon-lb\", \"QK-video\", \"Jester\", \"ML-10M\", \"ML-20M\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rel_item_df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
