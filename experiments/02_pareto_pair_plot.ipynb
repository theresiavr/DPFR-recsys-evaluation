{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data from combined_result\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "newest_file = #insert newest file\n",
    "list_dataset = [\n",
    "                \"Lastfm\",\n",
    "                \"Amazon-lb\", \n",
    "                \"QK-video\",\n",
    "                \"Jester\",\n",
    "                \"ML-10M\",\n",
    "                \"ML-20M\"\n",
    "                ]\n",
    "\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "\n",
    "list_color = ['#029e73', '#d55e00', '#cc78bc', '#ca9161'] #from colorblind palette\n",
    "list_rerank = [\"none\", \"BC\", \"CM\", \"GS\"]\n",
    "dict_color = dict(zip(list_rerank, list_color)) \n",
    "list_models = [\"BPR\", \"ItemKNN\", \"MultiVAE\", \"NCL\"]\n",
    "list_markers = [\"x\", \"1\", \"+\", \"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, est=None):\n",
    "    if not est:\n",
    "        list_files = [f for f in os.listdir(f\"../pareto/{file_path}\") if \"pickle\" in f and \"dftest\" not in f and \"10\" in f and \"temp\" not in f and \"time\" not in f and \"oraclefair\" in f]\n",
    "    elif est:\n",
    "        list_files = [f for f in os.listdir(f\"../pareto/{file_path}\") if \"pickle\" in f and \"dftest\" not in f and \"10\" in f and \"temp\" not in f and \"time\" not in f and \"oraclefair\" in f and \"pareto\" not in f and f\"with{est}.\" in f]\n",
    "        #the dot is important to distinguish with1.pickle vs with10.pickle\n",
    "    \n",
    "    assert(len(list_files) == len(list_dataset), print(len(list_files)))\n",
    "    big_df = pd.DataFrame()\n",
    "\n",
    "    for f in list_files:\n",
    "        df = pd.read_pickle(f\"../pareto/{file_path}/{f}\")\n",
    "\n",
    "        splitted_f = f.split(\"_\")\n",
    "        df[\"dataset\"] = splitted_f[2]\n",
    "\n",
    "\n",
    "        big_df = pd.concat([big_df, df])\n",
    "\n",
    "    rel_measures = big_df.columns[(~big_df.columns.str.contains(\"our|ori|dataset\"))&(big_df.columns.str.contains(\"@\", regex=False))].to_list()\n",
    "    fair_measures = big_df.columns[(big_df.columns.str.contains(\"Jain|QF|Ent|Gini|FSat\"))&(big_df.columns.str.contains(\"our\"))].to_list()\n",
    "\n",
    "    big_df[\"source\"] = \"pareto\"\n",
    "\n",
    "    model_scores = pd.read_csv(f\"combined_base/csv_combined_result_{newest_file}.csv\", index_col=0)\n",
    "    model_scores.sort_values([\"dataset\",\"reranking\"], inplace=True)\n",
    "    model_scores = model_scores.melt(id_vars=[\"dataset\", \"measures\", \"reranking\"]).set_index(\"measures\")\n",
    "    model_scores.index += \"@10\"\n",
    "    model_scores[\"source\"] = model_scores.apply(lambda x: x.variable + \"-\" + x.reranking if x.reranking != \"-\" else x.variable, axis=1)\n",
    "    model_scores.drop(columns=[\"reranking\", \"variable\"], inplace=True)\n",
    "\n",
    "    model_scores = model_scores\\\n",
    "                    .reset_index()\\\n",
    "                    .pivot_table(index=[\"measures\"], columns=[\"dataset\",\"source\"])\\\n",
    "                    .T\\\n",
    "                    .reset_index()\n",
    "\n",
    "    model_scores.drop(columns=\"level_0\", inplace=True)\n",
    "\n",
    "    model_scores[\"dataset\"] = model_scores.dataset.str.replace(\"\\\\rotatebox[origin=r]{90}{\",\"\", regex=False).str.rstrip(\"}\")\n",
    "\n",
    "    selected_cols = ['HR@10', 'MRR@10', 'P@10', 'MAP@10', 'R@10', 'NDCG@10', 'Jain_our@10','QF_our@10', 'Ent_our@10', 'Gini_our@10', 'FSat_our@10', \n",
    "                     'dataset', 'source']\n",
    "    \n",
    "    \n",
    "    combined_df = big_df.append(\n",
    "        model_scores.loc[:,selected_cols]\n",
    "    )\n",
    "\n",
    "    return combined_df, model_scores, rel_measures, fair_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To annotate best rel, best fair, and best distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_rel_best_fair(this_data):\n",
    "    for_val = this_data.loc[this_data.source!=\"pareto\"]\n",
    "\n",
    "    max_val = for_val.loc[:, ~for_val.columns.str.contains(\"Gini|dataset|source\")].max()\n",
    "    min_val = for_val.loc[:,for_val.columns.str.contains(\"Gini\")].min()\n",
    "\n",
    "    best_rel_or_fair = {}\n",
    "\n",
    "    for measure, val in zip(max_val.index, max_val):\n",
    "        best_rel_or_fair[measure] = for_val.loc[for_val[measure]==val, \"source\"].to_list()\n",
    "\n",
    "    for measure, val in zip(min_val.index, min_val):\n",
    "        best_rel_or_fair[measure] = for_val.loc[for_val[measure]==val, \"source\"].to_list()\n",
    "\n",
    "    return best_rel_or_fair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pareto gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pareto_gradient(combined_df, dataset):\n",
    "    the_data = combined_df.query(\"dataset==@dataset & source=='pareto'\").drop(columns=[\"source\", \"dataset\"])\n",
    "\n",
    "    selected_col = rel_measures + fair_measures\n",
    "\n",
    "    the_data = the_data.loc[:,selected_col]\n",
    "\n",
    "    for i in [0,-1]:  \n",
    "        print(i)\n",
    "        assert all((the_data.iloc[i] == the_data.min()) | (the_data.iloc[i] == the_data.max())), print(the_data.shape, the_data.idxmax(), the_data.idxmin()) #check if they are really the end points\n",
    "\n",
    "    start_point = the_data.iloc[0] #most ideal relevance\n",
    "    end_point = the_data.iloc[-1] #most fair\n",
    "    differences = start_point - end_point\n",
    "\n",
    "    series_gradient = pd.Series()\n",
    "\n",
    "    for fm in fair_measures:\n",
    "        for rm in rel_measures:\n",
    "            series_gradient.loc[f\"{rm}-{fm}\"] = differences[fm] / differences[rm]\n",
    "\n",
    "    series_gradient.index = series_gradient.index.str.replace(\"@10\",\"\")\n",
    "    series_gradient = series_gradient.sort_index()\n",
    "\n",
    "    return series_gradient\n",
    "\n",
    "def detect_good(el):\n",
    "    if np.isinf(el) or np.isnan(el):\n",
    "        return False\n",
    "    elif el==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def conclude(number):\n",
    "    if number==len(list_dataset):\n",
    "        return \"always good\"\n",
    "    elif number==0:\n",
    "        return \"always bad\"\n",
    "    else:\n",
    "        return \"inconsistent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(df1, df2, cols=['x_coord','y_coord']):\n",
    "    return np.linalg.norm(df1[cols].values - df2[cols].values, axis=1)\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx], idx\n",
    "\n",
    "def find_nearest_all(df, alpha=0.5):\n",
    "    result_dict = {}\n",
    "    rel_measures = df.columns[~df.columns.str.contains(\"our|dataset|ori\")]\n",
    "    fair_measures = df.columns[df.columns.str.contains(\"our|ori\")]\n",
    "\n",
    "    for rel_measure in rel_measures:\n",
    "        result_dict[rel_measure] = {}\n",
    "        for fair_measure in fair_measures:\n",
    "\n",
    "\n",
    "            df_copy = df[[rel_measure, fair_measure]]\\\n",
    "                                                    .copy()\\\n",
    "                                                    .sort_values(rel_measure, ascending=False, kind=\"stable\")\\\n",
    "                                                    .drop_duplicates(rel_measure, keep=\"last\") \n",
    "            #we keep last because fairness would be higher at the last index\n",
    "\n",
    "            #if both measures have 0 std, get their value instead\n",
    "            if ~df_copy.describe().loc[\"std\"].any():\n",
    "                val_rel = df_copy[rel_measure].unique()\n",
    "                val_fair = df_copy[fair_measure].unique()\n",
    "                assert val_rel.shape[0] == 1\n",
    "                assert val_fair.shape[0] == 1\n",
    "\n",
    "                result_dict[rel_measure][fair_measure] = np.array(val_rel[0], val_fair[0])\n",
    "                print(f\"Allocating the only values for {rel_measure}, {fair_measure}\")\n",
    "\n",
    "            else:\n",
    "                df1 = df_copy.iloc[:-1]\n",
    "                df2 = df_copy.shift(-1).iloc[:-1]\n",
    "\n",
    "                dist = euclidean_dist(df1, df2, cols=df1.columns)\n",
    "                # print(len(dist), len(df_copy))\n",
    "                cumsumdist = np.cumsum(dist) #this distance is from the right side (oracle), because we start from the point with highest rel\n",
    "\n",
    "                alpha_distance = alpha*cumsumdist[-1]\n",
    "\n",
    "                _, idx = find_nearest(cumsumdist, alpha_distance)\n",
    "                #this df2.iloc[idx].values is the point *in* the PF that is closest to alpha*distance\n",
    "                result_dict[rel_measure][fair_measure] = df2.iloc[idx].values\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def get_model_distance(df, path_integral_point, return_all_dist=False):\n",
    "    best_model = {}\n",
    "    for rel_measure in path_integral_point.columns:\n",
    "        best_model[rel_measure] = {}\n",
    "        for fair_measure in path_integral_point.index:\n",
    "            df1 = df[[rel_measure, fair_measure]]\n",
    "\n",
    "            #check na column:\n",
    "            df1 = df1.dropna()\n",
    "            df2 = path_integral_point.loc[fair_measure, rel_measure]\n",
    "\n",
    "            if len(df1)==0:\n",
    "                best_model[rel_measure][fair_measure] = (\"-\", np.nan)\n",
    "                continue\n",
    "\n",
    "\n",
    "            dist = np.linalg.norm(df1-df2, axis=1)\n",
    "            if not return_all_dist:\n",
    "                idx = dist.argmin()\n",
    "                best_model[rel_measure][fair_measure] = (df.iloc[idx][\"source\"], df1.iloc[idx].values)\n",
    "            else:\n",
    "                best_model[rel_measure][fair_measure] = (df.iloc[np.argsort(dist, kind=\"stable\")][\"source\"].values, np.sort(dist, kind=\"stable\"))\n",
    "                \n",
    "    best_model = pd.DataFrame(best_model)\n",
    "    return best_model\n",
    "\n",
    "def get_model_distance_dict(combined_df):\n",
    "    selected_merged = combined_df.query(\"source=='pareto'\").drop(columns=\"source\")\n",
    "\n",
    "    model_distance_dict = {}\n",
    "    path_integral_point_dict = {}\n",
    "\n",
    "    for data in combined_df.dataset.unique():\n",
    "        if data == \"pareto\":\n",
    "            continue\n",
    "\n",
    "        print(data)\n",
    "        this_data = selected_merged.query(\"dataset==@data\")\n",
    "\n",
    "        if len(this_data) == 0:\n",
    "            continue\n",
    "\n",
    "        path_integral_dict = find_nearest_all(this_data)\n",
    "        path_integral_point = pd.DataFrame(path_integral_dict)\n",
    "\n",
    "        model_distance = get_model_distance(\n",
    "                combined_df.query(\"dataset==@data & source!='pareto'\"), path_integral_point, return_all_dist=True\n",
    "                )\n",
    "\n",
    "        # display(model_distance)\n",
    "\n",
    "        model_distance_dict[data] = model_distance\n",
    "        path_integral_point_dict[data] = path_integral_point\n",
    "    \n",
    "    return model_distance_dict, path_integral_point_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine distance and pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_model_to_pareto(dataset: str, model_distance_dict):\n",
    "    model_distance_for_data = model_distance_dict[dataset].applymap(lambda x:x[1])\n",
    "    model_distance_for_data = model_distance_for_data.dropna()\n",
    "    assert all(\n",
    "        model_distance_for_data.applymap(lambda x: np.unique(x).shape == x.shape)\n",
    "    )\n",
    "    model_distance_for_data = model_distance_dict[dataset].applymap(lambda x:x[0])\n",
    "    \n",
    "    df_of_closest_models = model_distance_for_data[model_distance_for_data.applymap(len)>1]\\\n",
    "                                                                .dropna()\\\n",
    "                                                                .applymap(lambda x: x[0])\n",
    "    \n",
    "    df_of_closest_models = pd.DataFrame(df_of_closest_models.unstack())\n",
    "\n",
    "    return df_of_closest_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_avg(this_data):\n",
    "\n",
    "    for_val = this_data.loc[this_data.source!=\"pareto\"]\n",
    "    for_val_rel = for_val[rel_measures]\n",
    "    for_val_fair = for_val[fair_measures]\n",
    "    for_val_fair.loc[:,for_val_fair.columns.str.contains(\"Gini\")] = 1 - for_val_fair.loc[:,for_val_fair.columns.str.contains(\"Gini\")]\n",
    "\n",
    "    df_average = pd.DataFrame(columns=[\"rel\", \"fair\", \"score\", \"source\"])\n",
    "\n",
    "    for col in for_val_fair.columns:\n",
    "        avg_val_for_col = (for_val_rel.values + for_val_fair[col].values.reshape(-1,1))/2\n",
    "        df_avg_col = pd.DataFrame(avg_val_for_col, columns=rel_measures)\n",
    "        df_avg_col[\"source\"] = for_val.source.values\n",
    "        df_avg_col[\"fair\"] = col\n",
    "        melted = df_avg_col.melt([\"fair\", \"source\"], var_name=\"rel\", value_name=\"score\")\n",
    "        df_average = pd.concat([df_average, melted])\n",
    "\n",
    "    df_best_avg = pd.DataFrame(df_average.groupby([\"rel\",\"fair\"])[\"score\"].max())\n",
    "    df_best_avg[\"source\"] = pd.Series(dtype=object)\n",
    "\n",
    "    for x, row in df_best_avg.iterrows():\n",
    "        rel_now = x[0]\n",
    "        fair_now = x[1]\n",
    "        score_now = row.values[0]\n",
    "\n",
    "        model_list = df_average.query(\"rel==@rel_now & fair==@fair_now & score==@score_now\").source.values\n",
    "\n",
    "        assert len(model_list) <= 1\n",
    "\n",
    "        if len(model_list) == 1:\n",
    "            df_best_avg.at[x, \"source\"] = model_list[0]\n",
    "\n",
    "    return df_best_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations_from_plot(g):\n",
    "    result_dict = {}\n",
    "    for each_ax in g.axes.flatten():\n",
    "        xlabel = each_ax.get_xlabel()\n",
    "        ylabel = each_ax.get_ylabel()\n",
    "        result_dict[f\"{xlabel}-{ylabel}\"] = []\n",
    "        for child in each_ax.get_children():\n",
    "            if isinstance(child, matplotlib.text.Annotation):\n",
    "                result_dict[f\"{xlabel}-{ylabel}\"].append(child.get_text())\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return np.where((x>0),x**(1/2),x)\n",
    "\n",
    "def inverse(x):\n",
    "    return np.where((x>0) ,x**2 * np.sign(x),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='.', mec='none',label='Pareto-optimal solution',\n",
    "                          markerfacecolor=\"#0173b2\",markersize=13, linestyle='None')]\n",
    "\n",
    "\n",
    "\n",
    "def annotate_model_for_paper(the_model, xlabel, ylabel, thelabel, this_data, each_ax):\n",
    "    x_coord, y_coord = this_data.query(\"source==@the_model\")[[xlabel, ylabel]].values.flatten()\n",
    "\n",
    "    \n",
    "    each_ax.plot(x_coord, y_coord, 'o', ms=13, mec='black', mfc='none', mew=1)\n",
    "\n",
    "    ha = \"left\"\n",
    "\n",
    "    if \"Rel\" in thelabel:\n",
    "        offset = -5\n",
    "    elif \"Fair\" in thelabel:\n",
    "        offset = 8\n",
    "    elif \"Pareto\" == thelabel:\n",
    "        offset = -5\n",
    "    else:\n",
    "        offset = 0.1\n",
    "\n",
    "    \n",
    "    thelabel = thelabel\\\n",
    "                        .replace(\"Pareto\", \"DPFR\")\\\n",
    "                        .replace(\"Average\", \"Avg\")\\\n",
    "                        .replace(\"Relevance\",\"\\\\textsc{Rel}\")\\\n",
    "                        .replace(\"Fairness\",\"\\\\textsc{Fair}\")\n",
    "\n",
    "\n",
    "    each_ax.annotate(thelabel,\n",
    "                xy=(x_coord, y_coord),\n",
    "                xytext=(8, offset),\n",
    "                textcoords='offset points',\n",
    "                ha=ha, va='center',\n",
    "                fontsize = \"14\"\n",
    "                )\n",
    "\n",
    "def plot_pareto_for_paper(combined_df, model_distance_dict):\n",
    "    # plt.rcParams['figure.figsize']=(8.5,12)\n",
    "    plt.rcParams['text.latex.preamble'] = r'\\usepackage{sfmath} \\boldmath'\n",
    "    best_model_annotated = {}\n",
    "\n",
    "    for dataset in list_dataset:\n",
    "\n",
    "        this_data = combined_df.query(\"dataset==@dataset\")\n",
    "\n",
    "        select_data_now = this_data.loc[this_data.source.str.contains(\"BPR\")]\n",
    "        select_data_now[\"source\"] = select_data_now\\\n",
    "                                        .apply(lambda x: x.source+\"-none\" if \"-\" not in x.source else x.source, axis=1)\\\n",
    "                                        .apply(lambda x: x.split(\"-\")[1])\n",
    "        ax1 = sns.scatterplot(data=select_data_now, x=rel_measures[0], y=fair_measures[1], palette=list_color, hue=\"source\", hue_order=list_rerank)\n",
    "\n",
    "        data_fake_plot = this_data.loc[~this_data.source.str.contains(\"-|pareto\")]\n",
    "        ax2 = sns.scatterplot(data=data_fake_plot, x=rel_measures[0], y=fair_measures[1], style=\"source\", markers=list_markers)\n",
    "        handles, labels = ax2.get_legend_handles_labels()           \n",
    "        plt.close()\n",
    "\n",
    "        filtered_rel_measures = [x for x in rel_measures if x.strip(\"@10\") in [\"NDCG\", \"MAP\", \"R\", \"P\"]]\n",
    "        filtered_fair_measures =  [x for x in fair_measures if \"our\" in x]\n",
    "        filtered_fair_measures = [x for x in filtered_fair_measures if \"Ent\" in x or \"Gini\" in x or \"Jain\" in x]\n",
    "\n",
    "        assert len(filtered_rel_measures) == 4\n",
    "        assert len(filtered_fair_measures) == 3\n",
    "\n",
    "        g = sns.PairGrid(\n",
    "                        data=this_data,\n",
    "                        x_vars=filtered_rel_measures,\n",
    "                        y_vars=filtered_fair_measures,\n",
    "                        palette=dict_color,\n",
    "                        aspect=1.05\n",
    "                        )\n",
    "\n",
    "\n",
    "        #PLOT PARETO\n",
    "        pareto_data = this_data.query(\"source=='pareto'\")\n",
    "        g.data = pareto_data\n",
    "        g.map(sns.scatterplot, marker=\".\", linewidth=0.01, ec='none', color=\"#0173b2\")\n",
    "\n",
    "        #PLOT THE 4 MODELS SEPARATELY\n",
    "        for model, marker in zip(list_models, list_markers):\n",
    "            select_data_now = this_data.loc[this_data.source.str.contains(model)]\n",
    "            select_data_now[\"source\"] = select_data_now\\\n",
    "                                            .apply(lambda x: x.source+\"-none\" if \"-\" not in x.source else x.source, axis=1)\\\n",
    "                                            .apply(lambda x: x.split(\"-\")[1])\n",
    "            g.data = select_data_now\n",
    "            g.map(sns.scatterplot, marker=marker, s=75, alpha=1, palette=dict_color, hue=select_data_now.source, hue_order=list_rerank, linewidth=2)\n",
    "\n",
    "        #ANNOTATE BEST MODEL based on fairness/relevance/pareto/simple average\n",
    "        best_rel_best_fair = get_best_rel_best_fair(this_data)\n",
    "        best_avg = get_best_avg(this_data)\n",
    "        best_pareto =  get_closest_model_to_pareto(dataset, model_distance_dict)\n",
    "\n",
    "        g.figure.suptitle(f\"{dataset}\", y=1, fontsize=13)\n",
    "\n",
    "        for each_ax in g.axes.flatten():\n",
    "            xlabel = each_ax.get_xlabel()\n",
    "            ylabel = each_ax.get_ylabel()\n",
    "\n",
    "            if xlabel not in best_pareto.index.get_level_values(0) or ylabel not in best_pareto.index.get_level_values(1):\n",
    "                continue\n",
    "\n",
    "            best_rel_model = best_rel_best_fair[xlabel]\n",
    "            best_fair_model = best_rel_best_fair[ylabel]\n",
    "            best_avg_model = best_avg.loc[(xlabel, ylabel)].source\n",
    "            best_pareto_model = best_pareto.loc[(xlabel, ylabel)].values[0]\n",
    "\n",
    "            avg_flag = False\n",
    "            pareto_flag = False\n",
    "\n",
    "            annotate_model_kws = dict(xlabel=xlabel, ylabel=ylabel, this_data=this_data, each_ax=each_ax)\n",
    "\n",
    "            if len(best_rel_model) == 1:\n",
    "                the_model = best_rel_model[0]\n",
    "                label = \"Relevance\"\n",
    "                if best_avg_model == the_model:\n",
    "                    label += \" \\& Average\"\n",
    "                    avg_flag = True\n",
    "                if best_pareto_model == the_model:\n",
    "                    label += \" \\& Pareto\"\n",
    "                    pareto_flag = True\n",
    "\n",
    "                annotate_model_for_paper(the_model, thelabel=label, **annotate_model_kws)\n",
    "\n",
    "            if len(best_fair_model) == 1:\n",
    "                the_model = best_fair_model[0]\n",
    "                label = \"Fairness\"\n",
    "                if best_avg_model == the_model:\n",
    "                    label += \" \\& Average\"\n",
    "                    avg_flag = True\n",
    "                if best_pareto_model == the_model:\n",
    "                    label += \" \\& Pareto\"\n",
    "                    pareto_flag = True\n",
    "                annotate_model_for_paper(the_model, thelabel=label, **annotate_model_kws)\n",
    "\n",
    "\n",
    "            if best_avg_model == best_pareto_model and not pareto_flag:\n",
    "                label = \"Average \\& Pareto\"\n",
    "                annotate_model_for_paper(best_avg_model, thelabel=label,  **annotate_model_kws)\n",
    "                avg_flag = True\n",
    "                pareto_flag = True\n",
    "\n",
    "            if not avg_flag:\n",
    "                label = \"Average\"\n",
    "                annotate_model_for_paper(best_avg_model, thelabel=label, **annotate_model_kws)\n",
    "            \n",
    "            if not pareto_flag:\n",
    "                label = \"Pareto\"\n",
    "                annotate_model_for_paper(best_pareto_model, thelabel=label, **annotate_model_kws)\n",
    "\n",
    "            each_ax.set_xscale('function', functions=(forward, inverse))\n",
    "            \n",
    "            ylabel = ylabel.replace(\"_our\",\"\")\n",
    "\n",
    "            # inverted Gini\n",
    "            if \"Gini\" not in ylabel:\n",
    "                each_ax.set_yscale('function', functions=(forward, inverse))\n",
    "                each_ax.set_ylabel(\"\\\\textbf{$\\\\uparrow$\" + ylabel+\"}\", fontsize=13)\n",
    "            else:\n",
    "                each_ax.set_yscale('function', functions=(inverse, forward))\n",
    "                each_ax.set_ylabel(\"\\\\textbf{$\\\\downarrow$\" + ylabel+\"}\", fontsize=13)\n",
    "                each_ax.yaxis.set_inverted(True) \n",
    "\n",
    "            each_ax.set_xlabel(\"\\\\textbf{$\\\\uparrow$\" + xlabel+\"}\", fontsize=13)\n",
    "            each_ax.tick_params(axis='both', which='major', labelsize=9)\n",
    "            each_ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))\n",
    "            each_ax.yaxis.set_major_locator(ticker.MaxNLocator(nbins=5))\n",
    "\n",
    "        best_model_annotated[dataset] = get_annotations_from_plot(g)\n",
    "\n",
    "        argspareto = dict( loc=\"upper center\",handletextpad=0.01, fontsize=14, labelspacing=0.2)\n",
    "\n",
    "        if dataset in [\"Lastfm\", \"Amazon-lb\"]:\n",
    "            g.figure.legend(handles[-4:-1]+handles[-1:], labels[-4:-1]+labels[-1:], title=\"Model\", ncol=4, bbox_to_anchor=(0.15, 1.075), columnspacing=0.15, markerscale=1.5, **argspareto\n",
    "                       )\n",
    "            g.figure.legend(handles[-8:-4], labels[-8:-4], title=\"Re-ranker\",  ncol=4, bbox_to_anchor=(0.488,1.075), columnspacing=0.15, **argspareto\n",
    "                       )\n",
    "            g.figure.legend(handles=legend_elements, title=\"Pareto Frontier\",  ncol=1, bbox_to_anchor=(0.85,1.075), **argspareto)\n",
    "            \n",
    "        plt.subplots_adjust(hspace = 0.05, wspace=0.11)\n",
    "\n",
    "        plt.savefig(f'pairplot/pairplot_{dataset.replace(\"-\", \"\").lower()}.png', bbox_inches=\"tight\") # uncomment this one to save\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return best_model_annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_full, model_scores, rel_measures, fair_measures = load_data(\"result_combined\")\n",
    "df_gradient = pd.DataFrame(columns=list_dataset)\n",
    "\n",
    "\n",
    "for data in list_dataset:\n",
    "    print(f\"Doing {data}\")\n",
    "    gradient_for_data = get_pareto_gradient(combined_df_full, data)\n",
    "    df_gradient[data] = gradient_for_data\n",
    "\n",
    "df_gradient = df_gradient.round(2)\n",
    "df_gradient = df_gradient[df_gradient.index.str.contains(\"our\")]\n",
    "df_gradient.index = df_gradient.index.str.replace(\"\\_our\",\"\")\n",
    "df_gradient[\"\\# good\"] = df_gradient.applymap(detect_good).sum(axis=1)\n",
    "df_gradient[\"conclusion\"] = df_gradient[\"\\# good\"].apply(lambda num: conclude(num))\n",
    "df_gradient = df_gradient.fillna(\"-\").replace(np.inf,\"-\").replace(-np.inf,\"-\")\n",
    "print(\n",
    "    df_gradient.to_latex(escape=False)\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pairs = df_gradient.query(\"conclusion=='always good'\").index\n",
    "good_pairs = good_pairs.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_full = combined_df_full.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distance_dict_full, path_integral_point_full = get_model_distance_dict(combined_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"efficiency/path_integral_point_full.pickle\",\"wb\") as f:\n",
    "    pickle.dump(path_integral_point_full, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"efficiency/model_distance_dict_full.pickle\",\"wb\") as f:\n",
    "    pickle.dump(\n",
    "        model_distance_dict_full, \n",
    "        f, \n",
    "        pickle.HIGHEST_PROTOCOL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "best_model_annotated_full = plot_pareto_for_paper(combined_df_full, model_distance_dict_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get best model stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_best_model(best_model_annotated):\n",
    "    new_best_model_annotated = {}\n",
    "    for key, val in best_model_annotated.items():\n",
    "        new_best_model_annotated[key] = {}\n",
    "\n",
    "        for measure, alist in val.items():\n",
    "            new_measure = measure.replace(\"$\\\\uparrow$\",'').replace(\"$\\\\downarrow$\",\"\")\n",
    "            new_best_model_annotated[key][new_measure] = alist\n",
    "\n",
    "    print(new_best_model_annotated)\n",
    "\n",
    "    df_best_model = pd.DataFrame(new_best_model_annotated)\n",
    "    df_best_model = df_best_model\\\n",
    "        .applymap(lambda x: [el for el in x if \"Avg\" in el or \"DPFR\" in el])\\\n",
    "        .applymap(lambda x: [el.replace(\"Best \", \"\") for el in x])\n",
    "\n",
    "    return df_best_model\n",
    "\n",
    "\n",
    "def create_comparison(df_best_model):\n",
    "    total_valid_pair = df_best_model.applymap(lambda x: len(x)>0).astype(int).sum()\n",
    "    same_average_and_pareto = df_best_model.applymap(lambda x: [el for el in x if \"Avg\" in el and \"DPFR\" in el]).applymap(len)\n",
    "    \n",
    "    set_based = same_average_and_pareto.index[~same_average_and_pareto.index.str.contains(\"MAP|NDCG\")]\n",
    "    rank_based = same_average_and_pareto.index[same_average_and_pareto.index.str.contains(\"MAP|NDCG\")]\n",
    "\n",
    "    same_set_based = same_average_and_pareto.loc[set_based].sum()\n",
    "    same_rank_based = same_average_and_pareto.loc[rank_based].sum()\n",
    "\n",
    "    num_set_based = set_based.shape[0]\n",
    "    num_rank_based = rank_based.shape[0]\n",
    "\n",
    "    disagreement_best = pd.DataFrame([same_set_based, same_rank_based]).T\n",
    "    disagreement_best.columns = [\"Set-based\", \"Rank-based\"]\n",
    "\n",
    "\n",
    "    disagreement_best[\"All\"] = 100 * (len(same_average_and_pareto) - disagreement_best[\"Set-based\"] - disagreement_best[\"Rank-based\"]) / len(same_average_and_pareto)\n",
    "    disagreement_best[\"Set-based\"] = 100 * (num_set_based - disagreement_best[\"Set-based\"]) / num_set_based \n",
    "    disagreement_best[\"Rank-based\"] = 100 * (num_rank_based - disagreement_best[\"Rank-based\"]) / num_rank_based \n",
    "\n",
    "    num_data = disagreement_best.shape[0]\n",
    "\n",
    "    set_based_all = 100* (num_data*num_set_based - same_set_based.sum()) / (num_data*num_set_based )\n",
    "    rank_based_all = 100*(num_data*num_rank_based - same_rank_based.sum()) / (num_data*num_rank_based)\n",
    "    all_measures_all_dataset = 100*(num_data*len(same_average_and_pareto) - same_set_based.sum() - same_rank_based.sum() ) / (num_data*len(same_average_and_pareto))\n",
    "\n",
    "    disagreement_best.loc[\"All datasets\"] = [set_based_all, rank_based_all, all_measures_all_dataset]\n",
    "    disagreement_best = disagreement_best.round(2)\n",
    "\n",
    "\n",
    "    print(\n",
    "        disagreement_best.to_latex(\n",
    "            escape=False,\n",
    "            label=\"tab:best_model_agreement\",\n",
    "            caption=\"\")\n",
    "    )\n",
    "\n",
    "\n",
    "def get_best_model_stat(best_model_annotated):\n",
    "    df_best_model = get_df_best_model(best_model_annotated)\n",
    "    df_best_model.index =  df_best_model.index.str.replace(\"@10\",\"\")\n",
    "    df_best_model = df_best_model.loc[good_pairs]\n",
    "\n",
    "    create_comparison(df_best_model)\n",
    "    return df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_model_full = get_best_model_stat(best_model_annotated_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rank_based_on_measure = {}\n",
    "all_rank_based_on_distance = {} \n",
    "\n",
    "\n",
    "def get_avg(this_data):\n",
    "\n",
    "    for_val = this_data.loc[this_data.source!=\"pareto\"]\n",
    "    for_val_rel = for_val[rel_measures]\n",
    "    for_val_fair = for_val[fair_measures]\n",
    "    for_val_fair.loc[:,for_val_fair.columns.str.contains(\"Gini\")] = 1 - for_val_fair.loc[:,for_val_fair.columns.str.contains(\"Gini\")]\n",
    "\n",
    "    df_average = pd.DataFrame(columns=[\"rel\", \"fair\", \"score\", \"source\"])\n",
    "\n",
    "    for col in for_val_fair.columns:\n",
    "        avg_val_for_col = (for_val_rel.values + for_val_fair[col].values.reshape(-1,1))/2\n",
    "        df_avg_col = pd.DataFrame(avg_val_for_col, columns=rel_measures)\n",
    "        df_avg_col[\"source\"] = for_val.source.values\n",
    "        df_avg_col[\"fair\"] = col\n",
    "        melted = df_avg_col.melt([\"fair\", \"source\"], var_name=\"rel\", value_name=\"score\")\n",
    "        df_average = pd.concat([df_average, melted])\n",
    "\n",
    "    return df_average\n",
    "\n",
    "\n",
    "def distance_based_rank_for_corr(model_distance_dict, data):\n",
    "    rank_based_on_distance = model_distance_dict[data].unstack().reset_index()\n",
    "    rank_based_on_distance.columns = [\"rel\",\"fair\",\"models\"]\n",
    "    rank_based_on_distance = rank_based_on_distance.loc[rank_based_on_distance.rel.str.contains(\"^P|^R|NDCG|MAP\")]\n",
    "    rank_based_on_distance = rank_based_on_distance.loc[rank_based_on_distance.fair.str.contains(\"Jain|Gini|Ent\")]\n",
    "    rank_based_on_distance = rank_based_on_distance.loc[rank_based_on_distance.fair.str.contains(\"our\")]\n",
    "    rank_based_on_distance = rank_based_on_distance.loc[rank_based_on_distance.models.apply(lambda x: x[1]).dropna().index]\n",
    "    rank_based_on_distance[\"col_name\"] = rank_based_on_distance.rel + \"-\" + rank_based_on_distance.fair\n",
    "    rank_based_on_distance = rank_based_on_distance[[\"col_name\",\"models\"]].T\n",
    "    rank_based_on_distance.columns = rank_based_on_distance.loc[\"col_name\"]\n",
    "    rank_based_on_distance = rank_based_on_distance.iloc[1].T\n",
    "    \n",
    "    dict_rank_based_on_distance = {}\n",
    "\n",
    "    for row, item in pd.DataFrame(rank_based_on_distance).iterrows():\n",
    "        the_tup = item[0]\n",
    "        model_name = the_tup[0]\n",
    "        scores = the_tup[1]\n",
    "        dict_rank_based_on_distance[row] = dict((key,val) for key,val in zip(model_name, scores))\n",
    "\n",
    "\n",
    "    for_corr = pd.DataFrame(dict_rank_based_on_distance).T.applymap(lambda x: -x)\n",
    "\n",
    "    return for_corr, dict_rank_based_on_distance\n",
    "\n",
    "\n",
    "def plot_corr_heatmap(combined_df, model_distance_dict):\n",
    "    fig, ax = plt.subplots(nrows=len(list_dataset),figsize=(8,16))\n",
    "\n",
    "    for ax_id, data in zip(ax.flatten(), list_dataset):\n",
    "\n",
    "        #rank based on measure\n",
    "        rank_based_on_measure = {}\n",
    "        model_scores_for_data = model_scores.query(\"dataset==@data\")\n",
    "\n",
    "        for col in model_scores_for_data.columns:\n",
    "            if col in [\"dataset\", \"source\"]:\n",
    "                pass\n",
    "            else:\n",
    "                if bool(re.search('AI|II|IAA|MME', col)):\n",
    "                    sorted = model_scores_for_data.sort_values(col, ascending=True, kind=\"stable\")\n",
    "                    print(f\"sorting {col} ascendingly\")\n",
    "                elif bool(re.search('IBO', col)):\n",
    "                    sorted = model_scores_for_data.sort_values(col, ascending=False, kind=\"stable\")\n",
    "                    print(f\"sorting {col} descendingly\")\n",
    "                else:\n",
    "                    continue\n",
    "                rank_based_on_measure[col] = sorted.source.values\n",
    "\n",
    "        rank_based_on_measure = pd.DataFrame(rank_based_on_measure)\n",
    "        all_rank_based_on_measure[data] = rank_based_on_measure\n",
    "\n",
    "        for_corr = model_scores_for_data.loc[:,model_scores_for_data.columns.str.contains(\"AI|II|IAA|IBO|MME|source\")]\n",
    "        for_corr = for_corr.loc[:, ~for_corr.columns.str.contains(\"IBO_ori|IWO_ori\")]\n",
    "        for_corr = for_corr.T\n",
    "        for_corr.columns = for_corr.loc[\"source\"]\n",
    "        for_corr.drop(index=[\"source\"], inplace=True)\n",
    "\n",
    "        for_corr.loc[for_corr.index.str.contains(\"AI|IAA|II|MME\")] = for_corr.loc[for_corr.index.str.contains(\"AI|IAA|II|MME\")].apply(lambda x: -x)\n",
    "\n",
    "        to_append, dict_rank_based_on_distance = distance_based_rank_for_corr(model_distance_dict, data)\n",
    "        for_corr_appended = for_corr.append(to_append)\n",
    "        all_rank_based_on_distance[data] = dict_rank_based_on_distance\n",
    "\n",
    "        #avg = higher score is better, so no need to invert\n",
    "        this_data = combined_df.query(\"dataset==@data\")\n",
    "        avg = get_avg(this_data)\n",
    "        avg[\"rel_fair\"] = avg[\"rel\"] + \"-\" + avg[\"fair\"]\n",
    "        avg = avg.drop(columns=[\"rel\", \"fair\"])\n",
    "        avg = avg.set_index(\"rel_fair\")\n",
    "\n",
    "        avg.index = avg.index.str.replace(\"@10\",\"\") + \"-avg\"\n",
    "        for_corr_appended = for_corr_appended.append(avg.pivot(columns=\"source\", values=\"score\"))\n",
    "\n",
    "\n",
    "        #plotting\n",
    "        to_plot = for_corr_appended.T.reset_index(drop=True).applymap(float).corr(method=\"kendall\").round(2)\n",
    "        to_plot.dropna(how=\"all\",inplace=True, axis=1)\n",
    "        to_plot.dropna(how=\"all\",inplace=True, axis=0)\n",
    "        to_plot.columns = to_plot.columns.str.replace(\"@10\", \"\")\n",
    "        to_plot.index = to_plot.index.str.replace(\"@10\",\"\")\n",
    "\n",
    "        #do the indexing automatically\n",
    "        idx_index = to_plot.columns.tolist().index(\"MME_ori\") +1\n",
    "        idx_col = to_plot.index.tolist().index(\"MME_ori\") +1\n",
    "\n",
    "\n",
    "        filtered = to_plot.iloc[idx_index:, :idx_col]\n",
    "        wo_avg = filtered[~filtered.index.str.contains(\"avg\")]\n",
    "        wo_avg[\"avg\"] = pd.Series()\n",
    "        only_avg = to_plot.loc[~to_plot.index.str.contains(\"AI|IAA|II|IBO|MME\"),to_plot.columns.str.contains(\"avg\")]\n",
    "\n",
    "        for pair in wo_avg.index:\n",
    "            wo_avg.loc[pair, \"avg\"] = only_avg.loc[pair, pair+\"-avg\"]\n",
    "\n",
    "        wo_avg = wo_avg.T\n",
    "        wo_avg.index = wo_avg.index\\\n",
    "                                .str.replace(\"_ori\",\"\")\\\n",
    "                                .str.replace(\"_our\",\"\")\\\n",
    "                                .str.replace(\"_true\",\"\")\n",
    "        wo_avg.columns = wo_avg.columns\\\n",
    "                                .str.replace(\"_ori\",\"\")\\\n",
    "                                .str.replace(\"_our\",\"\")\\\n",
    "\n",
    "        wo_avg = wo_avg.loc[[\"IBO\", \"MME\", \"IAA\", \"II-F\", \"AI-F\", \"avg\"]]\n",
    "\n",
    "        sns.heatmap(wo_avg,\n",
    "                    annot=True,\n",
    "                    cmap=\"coolwarm_r\",\n",
    "                    vmin = -1,\n",
    "                    vmax = 1,\n",
    "                    square = True,\n",
    "                    ax = ax_id,\n",
    "                    cbar = True,\n",
    "                    annot_kws={\"size\": 10}\n",
    "                    )\n",
    "        ax_id.set_yticklabels(ax_id.get_yticklabels(), rotation=0)\n",
    "        if data != list_dataset[-1]:\n",
    "           ax_id.set_xticklabels([])\n",
    "        ax_id.set_title(f\"{data}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'corr/corr_heatmap_all.pdf', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(combined_df_full, model_distance_dict_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table for DPFR scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_rank_based_on_distance = pd.DataFrame.from_dict(all_rank_based_on_distance, orient=\"columns\")\n",
    "df_all_rank_based_on_distance.to_pickle(\"combined_base/rank_based_distance.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto = pd.read_pickle(\"combined_base/rank_based_distance.pickle\")\n",
    "\n",
    "pareto.index = pareto.index.str.replace(\"@10\",\"\").str.replace(\"_our\",\"\")\n",
    "\n",
    "pareto = pareto.reset_index(names=[\"measure pair\"])\n",
    "pareto = pareto.melt(id_vars=\"measure pair\", var_name=\"dataset\")\n",
    "\n",
    "pareto = pd.concat([pareto, pareto.value.apply(pd.Series)], axis=1)\n",
    "\n",
    "pareto.drop(columns=\"value\", inplace=True)\n",
    "pareto = pareto.melt(id_vars=[\"dataset\",\"measure pair\"], var_name=[\"model\"])\n",
    "\n",
    "pareto[\"reranking\"] = pareto.model.str.split(\"-\").apply(lambda x: x[1] if len(x)==2 else \"-\")\n",
    "pareto.model = pareto.model.str.split(\"-\").apply(lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pareto.set_index([\"dataset\", \"measure pair\", \"model\", \"reranking\"]).unstack([2,3]).droplevel(0, axis=1)[[\"ItemKNN\",\"BPR\", \"MultiVAE\", \"NCL\"]]\n",
    "df_table = df_table.round(3)\n",
    "df_table = df_table.reindex([\"-\", \"BC\", \"CM\", \"GS\"],level=1, axis=1)\\\n",
    "                    .reindex([\"Lastfm\", \"Amazon-lb\", \"QK-video\",\"Jester\", \"ML-10M\", \"ML-20M\"], level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [\n",
    "            \"P-Jain\",\n",
    "            \"P-Ent\",\n",
    "            \"P-Gini\",\n",
    "            \"MAP-Jain\",\n",
    "            \"MAP-Ent\",\n",
    "            \"MAP-Gini\",\n",
    "            \"R-Jain\",\n",
    "            \"R-Ent\",\n",
    "            \"R-Gini\",\n",
    "            \"NDCG-Jain\",\n",
    "            \"NDCG-Ent\",\n",
    "            \"NDCG-Gini\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = df_table.reindex(order, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styler = df_table.style\n",
    "\n",
    "def highlight_min(x):\n",
    "    return np.where(x == np.nanmin(x.to_numpy()), f\"font-weight: bold;\", None)\n",
    "\n",
    "styler.apply(highlight_min, axis=1)\\\n",
    "    .format(formatter=\"{:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_code = styler.to_latex(\n",
    "    hrules=True, \n",
    "    clines=\"skip-last;data\",\n",
    "    convert_css=True, \n",
    "    label=\"\",\n",
    "    caption= \"\",\n",
    "    environment = \"table*\",\n",
    "    column_format = \"lll*{4}{r}|*{4}{r}|*{4}{r}|*{4}{r}\",\n",
    "    multicol_align = \"c|\"\n",
    "    )\n",
    "\n",
    "#erase last cline\n",
    "last_cline_starts = latex_code.find(\"\\\\cline\", -100,-1)\n",
    "last_cline_ends = latex_code.find(\"\\\\bottomrule\")\n",
    "latex_code = latex_code[:last_cline_starts] + latex_code[last_cline_ends:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latex_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
